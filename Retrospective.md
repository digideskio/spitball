Problem (WIP...)
================
There is no way to know *the context surrounding* a metric, only that it happened.  

Goal (WIP...)
=============
* To encode and expose context.
* To encode more context into metrics. With increasing the number of metrics you have to deal with.
* Less, more general metrics. More, specific context. 
* By encoding key pieces of context into metrics you can reduce the granularity that you have to log at.
* To extract context from the metrics that happened in the same context.
* Encoding more context in metrics will allow us to make alerts that have tighter variance ranges make them higher fidelity.
* Currently we view metrics (on librato) with almost no context. When we see them we have to know what should be there. 


Current Success
===============

We already have had moderate success simply by aggregating metrics on request_ids and using the l2met style logs. These uses fall into one of the following categories.

* [Real Time Request Retrieval](#retrival)
* [Partitioning Metrics by Request Context](#parition)
* [Cross Application Inspection](#crossapp)

<a name="retrival"/>
Real Time Request Retrieval 
---------------------------

By using request_ids in their logs users are able to quickly filter out just the metrics relevant to the request. No splunk required. 

### Case Study: Tracking Down Build Slowdowns

On build and packaging we have been trying to track down what is taking so long during builds. This task been difficult because our dashboards that tell us this information have such high variance that they barley communicate more then if a component is down or not.

After refactoring our metrics to more a semantically consistent format and using request_ids with spitball we have been able to quickly identify some new targets for optimization. Additionally, we are able to store much richer data in vacuole (picks up build output).

#### Requirements
* Request_ids
* L2Met style logs
* Archive in Postgres. 

#### Desires
* Be able to get log-runtime-metrics linked to requests.
* Better handling of metrics with the same name. 
* Meta-data about the metric. (when did it come in)?

### Case Study: Straw (used with Dashboard)

Trying to filter through logs to find ones that are relevant to the operation that you just performed is difficult. With this in mind Straw was created. 

Straw is a plugin for chrome dev tools that allows you to retrieve the metrics aggregated for the requests that were just made form your browser. Currently Webapps is using it to help track down performance issues.

#### Requirements
* Request_ids
* L2Met style logs
* Return the Request_id assigned to each request in a header of the response. 

#### Desires
* Better handling of metrics with the same name
* Meta-data about the metric. (when did it come in)?

<a name="partition"/>
Partitioning Metrics by Request Context
-------------------------------------------------------
Encoding user parameters to enable the user to later filter based on them. 

### Case Study: Git Fetch Caching

On build and packaging after tracking down one of our biggest offenders for dyno churn in codon (the fetch operation). We partitioned metrics on the operation being performed. This allowed us to recycle existing metrics using the extra context provided by spitball we are able to effectively monitor the efficacy of the fetch caching with little to no changes in logging in the rest of codon.

#### Requirements
* Request_ids
* L2Met style logs
* Archive in Postgres

<a name="crossapp"/>
Cross Application Inspection
----------------------------------------
By passing the Request_id that you created for your request to other services and using spitball for aggregation, we are able to see a much more complete picture of the lifecycle of the request.

### Case Study: API

Currently both B&P and Dashboard are working to improve performance. Spitball has exposes a much better picture of exactly *what* is taking so long for a given request. Having the metrics that are generated by API for free simply by passing our request_id to them has given us a much better idea of where to hunt down performance issues. Additionally we have much more useful feedback for API... Instead of saying "This vague request is slow" we can say when we make "this type of request this particular metric is slow".

#### Requirements
* Request_Ids
* L2Met style logs
* Add log drain to spitball to service that you are calling into

