Problem (work in progress)
================
I need the logs for a particular request. Currently the only way to do that is to dig through splunk or grep the log stream. 

Goal
=============
Expose the request primitive in a way that allows access to all the metrics relevant to it. 

Current Success
===============
We already have had moderate success simply by collecting metrics on request_ids and using the l2met style logs. These uses fall into one of the following categories.

* [Real Time Request Retrieval](#retrival)
* [Partitioning Metrics by Request Context](#partition)
* [Cross Application Inspection](#crossapp)

<a name="retrival"/>
Real Time Request Retrieval 
---------------------------

By using request_ids in their logs users are able to quickly filter out just the metrics relevant to the request. No splunk required. 

### Case Study: Tracking Down Build Slowdowns

On build and packaging we have been trying to track down what is taking so long during builds. This task been difficult because our dashboards that tell us this information have such high variance that they barley communicate more then if a component is down or not.

After refactoring our metrics to more a semantically consistent format and using request_ids with spitball we have been able to quickly identify some new targets for optimization. Additionally, we are able to store much richer data in vacuole (picks up build output).

#### Requirements
* Request_ids
* L2Met style logs
* Archive in Postgres. 

#### Desires
* Be able to get log-runtime-metrics linked to requests.
* Better handling of metrics with the same name. 
* Meta-data about the metric. (when did it come in)?

### Case Study: Straw (used with Dashboard)

Trying to filter through logs to find ones that are relevant to the operation that you just performed is difficult. With this in mind Straw was created. 

Straw is a plugin for chrome dev tools that allows you to retrieve the metrics aggregated for the requests that were just made form your browser. Currently Webapps is using it to help track down performance issues.

#### Requirements
* Request_ids
* L2Met style logs
* Return the Request_id assigned to each request in a header of the response. 

#### Desires
* Better handling of metrics with the same name
* Meta-data about the metric. (when did it come in)?

<a name="partition"/>
Partitioning Metrics by Request Content
-------------------------------------------------------
Content of the request was used to partition the metrics that were used to generate some sort of report.

### Case Study: Git Fetch Caching

On build and packaging after tracking down one of our biggest offenders for dyno churn in codon (the fetch operation). We partitioned metrics on the operation being performed. This allowed us to recycle existing metrics using the extra context provided by spitball we are able to effectively monitor the efficacy of the fetch caching with little to no changes in logging in the rest of codon.

#### Requirements
* Request_ids
* L2Met style logs
* Archive in Postgres

<a name="crossapp"/>
Cross Application Inspection
----------------------------------------
By passing the Request_id that that was created for the request to other services and using spitball for aggregation,  a much more complete picture of the lifecycle of the request is exposed.

### Case Study: API

Currently both B&P and Dashboard are working to improve performance. Spitball has exposes a much better picture of exactly *what* is taking so long for a given request. Having the metrics that are generated by API for free simply by passing our request_id to them has given us a much better idea of where to hunt down performance issues. Additionally we have much more useful feedback for API... Instead of saying "This vague request is slow" we can say when we make "this type of request this particular metric is slow".

#### Requirements
* Request_Ids
* L2Met style logs
* Add log drain to spitball to service that you are calling into

